{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "stdout = sys.stdout\n",
    "import numpy as np\n",
    "from numpy.random import shuffle\n",
    "sys.stdout = stdout\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        nodes,\n",
    "        step_size=0.0000000000001,\n",
    "        lmbda=0.001\n",
    "    ):\n",
    "        \"\"\"\n",
    "        nodes represents the number of nodes per layer. \n",
    "        eg: [2,3,5] is 2 feature input, \n",
    "        3 neurons in the first layer,\n",
    "        5 neurons in the final layer.\n",
    "        5 also represents the number of classes\n",
    "        \n",
    "        nb_layers includes both the output and input layers\n",
    "        weights contains the weights between each subsequent layer.\n",
    "        biases stores the biases with the same indexing as weights\n",
    "        outputs is the output matrix of each layer. An output matrix is of shape (p, n),\n",
    "            where p is the number of examples given to the feedforward, and n is the nb of nodes in the layer\n",
    "        gradients is the derivates of each layer. Each row is a different layer\n",
    "        deltas stores the update values for each layer\n",
    "        lmbda is the L2 regularization parameter.\n",
    "        \"\"\"\n",
    "        self.step_size = step_size\n",
    "        \n",
    "        self.nb_layers = len(nodes)\n",
    "        self.nodes = nodes\n",
    "        self.weights = [ np.zeros((n, m)) for n,m in zip(nodes[:-1], nodes[1:])]\n",
    "        for weight in self.weights:\n",
    "            weight.fill(0.001)\n",
    "        self.biases = [np.random.rand(1,n) for n in self.nodes[1:]]\n",
    "        self.outputs = [ 0 for n in nodes[1:]]\n",
    "        self.error_ds = np.zeros(nodes[-1])\n",
    "        self.gradients = [ 0 for n in nodes[1:]]\n",
    "        self.deltas = [ 0 for n in nodes[1: ]]\n",
    "        self.lmbda = lmbda\n",
    "    \n",
    "    def fforw(self, inputs, labels):\n",
    "        curr_input = inputs\n",
    "        for w in range(self.nb_layers-1):\n",
    "#             print(type(curr_input), type(self.weights[w]))\n",
    "#             print(\"inp\", curr_input)\n",
    "#             print(\"weights\", self.weights[w] )\n",
    "            next_inp = np.matmul(curr_input, self.weights[w])\n",
    "#             print(\"inp . weights\", next_inp)\n",
    "            next_inp += self.biases[w]\n",
    "#             print(trans_bias.shape, self.biases[w].shape)\n",
    "#             print(next_inp.shape)\n",
    "            next_act = self.sigmoid(next_inp)\n",
    "#             print(\"after adding bias\", next_inp)\n",
    "#             print(\"applied sigmoid\", next_act)\n",
    "            self.outputs[w] = next_act\n",
    "            curr_input = next_act\n",
    "    \n",
    "    def backprop(self, inputs, labels):\n",
    "        # using MSE loss, calculate error\n",
    "        err = np.zeros((len(labels), self.nodes[-1]), dtype=float)\n",
    "        for i,label in enumerate(labels):\n",
    "            expected = int(label[0])\n",
    "            err[i][expected] = label[0]\n",
    "#         print(len(self.outputs))\n",
    "#         print(err)\n",
    "        err = err - self.outputs[-1]\n",
    "#         print(err)\n",
    "#         print(self.outputs[-1])\n",
    "#         print(err.shape)\n",
    "        # calculate gradients\n",
    "        for i in range(self.nb_layers-1):\n",
    "            self.gradients[i] = self.sigmoid(self.outputs[i], True)\n",
    "        \n",
    "        # calculate delta\n",
    "        self.deltas[self.nb_layers-2] = err * self.gradients[self.nb_layers-2]\n",
    "        \n",
    "        for i in reversed(range(0, self.nb_layers-2)):\n",
    "#             print(self.deltas[i+1].shape, self.weights[i+1].shape, self.gradients[i].shape)\n",
    "            self.deltas[i] = (self.deltas[i+1].dot(self.weights[i+1].T)) * self.gradients[i]\n",
    "        \n",
    "        # update weights\n",
    "        for i in reversed(range(1, self.nb_layers-1)):\n",
    "            update = (-self.step_size)*self.outputs[i-1].T.dot(self.deltas[i])\n",
    "            self.weights[i] += update - (self.lmbda * self.weights[i])\n",
    "        self.weights[0] += (-self.step_size)*inputs.T.dot(self.deltas[0]) - (self.lmbda * self.weights[0])\n",
    "        \n",
    "        # update biases\n",
    "        for i in range(self.nb_layers-1):\n",
    "            bias_update = np.sum(self.deltas[i], axis=0) * (-self.step_size)\n",
    "#             print(\"bias\",self.biases[i].shape, bias_update.shape)\n",
    "            self.biases[i] = self.biases[i] + bias_update\n",
    "    \n",
    "    def train(self, inputs, labels):\n",
    "        self.fforw(inputs, labels)\n",
    "        self.backprop(inputs, labels)\n",
    "        \n",
    "    def relu(self, x, derivative=False):\n",
    "        if derivative:\n",
    "            x[x<=0.0] = 0.0\n",
    "            x[x>0.0] = 1.0\n",
    "            return x\n",
    "        else:\n",
    "            return x * (x > 0)\n",
    "    \n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        if derivative:\n",
    "            return x*(1.0-x)\n",
    "        else:\n",
    "            return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "    def tanh(x, derivative=False):\n",
    "        if derivative:\n",
    "            return (1.0 - (np.square(x)))\n",
    "        else:\n",
    "            return np.tanh(x)\n",
    "        \n",
    "    def fit(self, inputs, labels, epochs):\n",
    "        t_start = time.clock()\n",
    "        print(\"starting fit, time is: \", t_start)\n",
    "        t_last = time.clock()\n",
    "        for i in range(epochs):\n",
    "            print(\"epoch: \", i + 1)\n",
    "            # randomize data\n",
    "            print(np.shape(inputs), np.shape(labels))\n",
    "            state = np.random.get_state()\n",
    "            np.random.shuffle(inputs)\n",
    "            np.random.set_state(state)\n",
    "            np.random.shuffle(labels)\n",
    "            print(np.shape(inputs), np.shape(labels))\n",
    "#             self.update_weights(inputs, labels)\n",
    "            self.train(inputs, labels)\n",
    "            t_epoch = time.clock()\n",
    "            print(\"time elapsed is: \", t_epoch - t_last)\n",
    "            \n",
    "    def predict(self, inp, labels):\n",
    "        self.fforw(inp, labels)\n",
    "#         return self.outputs[len(self.nodes) - 2]\n",
    "#         print(\"outputs len: \", len(self.outputs))\n",
    "#         print(\"outputs\", self.outputs[-1])\n",
    "#         print(\"outputs\", self.outputs[self.nb_layers-2][0])\n",
    "        return [np.argmax(x) for x in self.outputs[self.nb_layers-2]]\n",
    "        #output of final layer\n",
    "\n",
    "# General functions\n",
    "\n",
    "def safe_divide(num, denom):\n",
    "    if len(num) != len(denom):\n",
    "        return []\n",
    "    return [num[i] / denom[i] if num[i] > 0.0 and denom[i] > 0.0 else 0.0 for i in range(len(num))]\n",
    "\n",
    "def score(preds, targets, nclasses):\n",
    "    # calculate fscore for each class, then take macro average (unweighted)\n",
    "    if (len(preds) != len(targets)):\n",
    "        return -1.0\n",
    "    true_pos = [0.0] * nclasses\n",
    "    pos = [0.0] * nclasses\n",
    "    rel_pos = [0.0] * nclasses\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] == targets[i]:\n",
    "            true_pos[int(targets[i])] += 1.0\n",
    "        pos[int(preds[i])] += 1.0\n",
    "        rel_pos[int(targets[i])] += 1.0\n",
    "    \n",
    "    true_pos = np.array(true_pos)\n",
    "    pos = np.array(pos)\n",
    "    rel_pos = np.array(rel_pos)\n",
    "    print(\"true_pos\", true_pos, \"\\n\", \"pos\", pos, \"\\n\", \"rel_pos\", rel_pos, \"\\n\")\n",
    "    p = safe_divide(true_pos, pos)\n",
    "    r = safe_divide(true_pos, rel_pos)\n",
    "    print(p, r)\n",
    "    \n",
    "    f_scores = [2.0 * p[i] * r[i] / (p[i] + r[i]) if p[i] + r[i] > 0.0 else 0.0 for i in range(len(p))]\n",
    "    \n",
    "    return np.mean(f_scores)\n",
    "\n",
    "def acc(preds, targets):\n",
    "    if (len(preds) != len(targets)):\n",
    "        return -1.0\n",
    "    correct = 0\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] == targets[i]:\n",
    "            correct += 1\n",
    "    return float(correct) / float(len(preds))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.clock()\n",
    "print(\"Loading train_x.npy, elapsed time: \", time.clock() - start_time)\n",
    "# x = np.loadtxt(\"../data/train_x.csv\", delimiter=\",\", dtype=int)\n",
    "x = np.load(\"x_centred_train.npy.npy\")\n",
    "print(\"Loaded train_x.csv. Loading train_y.csv, elapsed time: \", time.clock() - start_time)\n",
    "y = np.loadtxt(\"train_y.csv\", delimiter=\",\", dtype=int)\n",
    "print(y.shape)\n",
    "x = x.reshape(-1, 64*64)\n",
    "y = y.reshape(-1, 1)\n",
    "x[x<1.0] = 0\n",
    "x[x>1.0] = 1\n",
    "\n",
    "# print(type(y[0][0]), y[0].shape, y.shape)\n",
    "\n",
    "# binary_pics = (x > 250) + 0\n",
    "\n",
    "print(np.shape(x), np.shape(y))\n",
    "print(x[5000][2053], type(x[5000][2053]))\n",
    "\n",
    "#shuffle x and y\n",
    "state = np.random.get_state()\n",
    "np.random.shuffle(x)\n",
    "np.random.set_state(state)\n",
    "np.random.shuffle(y)\n",
    "\n",
    "#make train/valid split\n",
    "train_size = 25000\n",
    "valid_size = 25000\n",
    "end = train_size + valid_size\n",
    "train_x = x[0:train_size]\n",
    "train_y = y[0:train_size]\n",
    "valid_x = x[train_size: end]\n",
    "valid_y = y[train_size: end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "nhidden = [1, 2, 3]\n",
    "hiddensize = [1, 5, 50, 500, 2048, 4096]\n",
    "lmbdas = [0.00001, 0.0001, 0.001, 0.01, 0.1]\n",
    "learning_rates = [0.0000001, 0.00001, 0.001, 0.1]\n",
    "\n",
    "def tune(maxepochs, nhidden, hiddensize, lmbdas, learning_rates, train_x, train_y, test_x, test_y):\n",
    "    res = []\n",
    "    for lr in learning_rates:\n",
    "        for lmbda in lmbdas:\n",
    "            for layers in nhidden:\n",
    "                for size in hiddensize:\n",
    "                    nodes = [4096]\n",
    "                    for layer in range(layers):\n",
    "                        nodes.append(size)\n",
    "                    nodes.append(10)\n",
    "                    NN = Network(nodes, lmbda=lmbda, step_size=lr)\n",
    "                    for i in range(0, maxepochs):\n",
    "                        NN.fit(train_x, train_y, 1)\n",
    "                        preds = NN.predict(test_x, test_y)\n",
    "                        s = score(preds, test_y, 10)\n",
    "                        a = acc(preds, test_y)\n",
    "                        res.append((i, layers, size, lmbda, lr, s, a))\n",
    "                        if (len(res) % 50 == 0):\n",
    "                            np.save(\"nn-res-temp\", res)\n",
    "    return res\n",
    "\n",
    "\n",
    "# res = tune(epochs, nhidden, hiddensize, lmbdas, learning_rates, train_x, train_y, valid_x, valid_y)\n",
    "# np.save(\"nn-res\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# analyze saved res file\n",
    "res = np.load(\"nn-res-temp.npy\")\n",
    "res = list(res)\n",
    "res.sort(key=lambda x : x[5])\n",
    "print(res[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best accuracy\n",
    "# 2.00000000e+01 1.00000000e+00 5.00000000e+02 1.00000000e-04\n",
    "#  1.00000000e-07\n",
    "\n",
    "# best f1-score\n",
    "#2.20000000e+01 2.00000000e+00 4.09600000e+03 1.00000000e-02\n",
    "# 1.00000000e-07\n",
    "print(\"Training NN, elapsed time: \", time.clock() - start_time)\n",
    "\n",
    "# best performing model found, training below: :\n",
    "NN = Network([4096, 4096, 4096, 10], lmbda=1e-2, step_size=1e-7)\n",
    "NN.fit(train_x, train_y, 5)\n",
    "print(\"NN trained. elapsed time: \", time.clock() -start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions\n",
    "print(\"preds:\")\n",
    "preds = NN.predict(valid_x, valid_y)\n",
    "print(\"score:\")\n",
    "print(\"fscore: \", score(preds, valid_y, 10))\n",
    "print(\"acc: \", acc(preds, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save split data, optional\n",
    "# np.save(\"tune_train_x\", train_x)\n",
    "# np.save(\"tune_train_y\", train_y)\n",
    "# np.save(\"tune_valid_x\", valid_x)\n",
    "# np.save(\"tune_valid_y\", valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
