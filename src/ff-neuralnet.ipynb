{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "stdout = sys.stdout\n",
    "import numpy as np\n",
    "from numpy.random import shuffle\n",
    "sys.stdout = stdout\n",
    "import time\n",
    "print(\"started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        nodes,\n",
    "        step_size=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        nodes represents the number of nodes per layer. \n",
    "        eg: [2,3,5] is 2 feature input, \n",
    "        3 neurons in the first layer,\n",
    "        5 neurons in the final layer.\n",
    "        5 also represents the number of classes\n",
    "        \n",
    "        nb_layers includes both the output and input layers\n",
    "        outputs is the output matrix of each layer. An output matrix is of shape (p, n),\n",
    "            where p is the number of examples given to the feedforward, and n is the nb of nodes in the layer\n",
    "        derivates is the derivates of each layer. Each row is a different layer\n",
    "        errod_ds is the derivate of the error function\n",
    "        \n",
    "        Default cost function set to cross entropy, therefore, weights[-1] == nb of classes\n",
    "        \"\"\"\n",
    "        self.step_size = step_size\n",
    "        \n",
    "        self.nb_layers = len(nodes)\n",
    "        self.nodes = nodes\n",
    "        self.weights = [ np.ones((n+1, m), dtype=float) for n,m in zip(nodes[:-1], nodes[1:])]\n",
    "        self.outputs = [ 0 for n in nodes[1:]]\n",
    "        self.derivatives = [ 0 for n in nodes[1:]]\n",
    "        self.error_ds = np.zeros(nodes[-1])\n",
    "        self.gradients = [ 0 for n in nodes[1:]]\n",
    "        \n",
    "    def fforw(self, inputs, labels):\n",
    "        shape = np.shape(inputs)\n",
    "        cur_input = np.ones((shape[0], shape[1] + 1))\n",
    "        cur_input[:, :-1] = inputs\n",
    "        for l,w in enumerate(self.weights):\n",
    "#             print(np.shape(cur_input), np.shape(w))\n",
    "            out = self.sigmoid(np.dot(cur_input, w))\n",
    "#             print(l, out)\n",
    "            self.outputs[l] = out\n",
    "            self.derivatives[l] = self.sigmoid(out, True)\n",
    "            \n",
    "            shape = np.shape(out)\n",
    "            cur_input = np.ones((shape[0], shape[1] + 1))\n",
    "            cur_input[:, :-1] = out\n",
    "        \n",
    "        #Calculating the derivative of the error function for backprop\n",
    "        self.error_ds = self.softmaxLoss(cur_input[:, :-1], labels)\n",
    "        \n",
    "        #appending the input as output[-1] for future use\n",
    "        self.outputs.append(inputs)\n",
    "#         self.outputs.append([])\n",
    "        \n",
    "        return   \n",
    "\n",
    "    def backprop(self):\n",
    "        \"\"\"\n",
    "        Each gradient is of shape p x m,\n",
    "            where p is the number of examples, m is the number of output nodes from the layer\n",
    "        \"\"\"\n",
    "        self.gradients[-1] = self.derivatives[-1] * self.error_ds\n",
    "        \n",
    "        for i in range(1, len(self.derivatives)):\n",
    "            index = len(self.derivatives) - 1 - i\n",
    "            example_gradients = np.zeros(np.shape(self.derivatives[index]))\n",
    "            for j,example in enumerate(self.derivatives[index]):\n",
    "                example_gradients[j] = np.dot(np.diag(self.derivatives[index][j]), self.weights[index + 1][:-1]).dot(self.gradients[index+1][j])        \n",
    "            self.gradients[index] = example_gradients\n",
    "    \n",
    "    def update_weights(self, inputs, labels):\n",
    "        \"\"\"\n",
    "        inputs is the given input for the network.\n",
    "        Shape of inputs should be (n x m)\n",
    "            Where n is the number of examples,\n",
    "            m is the number of features\n",
    "        labels are the correct labels for each example of shape (n,)\n",
    "        \"\"\"\n",
    "        self.fforw(inputs, labels)\n",
    "        self.backprop()\n",
    "        for i,w in enumerate(self.weights):\n",
    "            shape = np.shape(self.outputs[i-1])\n",
    "            hat_o = np.ones((shape[0], shape[1] + 1))\n",
    "            hat_o [:, :-1] = self.outputs[i-1]\n",
    "            for e,g in enumerate(self.gradients[i]):\n",
    "                single_grad = self.gradients[i][e]\n",
    "                single_grad.shape = (len(single_grad), 1)\n",
    "                single_hat_o = hat_o[e]\n",
    "                single_hat_o.shape = (1, len(single_hat_o))\n",
    "                single_update = -self.step_size*(np.dot(single_grad, single_hat_o)).T\n",
    "                self.weights[i] += single_update\n",
    "    \n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        if derivative:\n",
    "            return x*(1-x)\n",
    "        else:\n",
    "            return 1/(1+np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def softmaxLoss(self, X, y):\n",
    "        if (len(y.shape) == 0):\n",
    "            m = 1\n",
    "        else:\n",
    "            m = y.shape[0]\n",
    "        p = self.softmax(X)\n",
    "#         log_likelihood = -np.log(p[range(m), y])\n",
    "#         loss = np.sum(log_likelihood) / m\n",
    "\n",
    "        dx = p.copy()\n",
    "        dx[range(m), y] -= 1\n",
    "        dx /= m\n",
    "        return dx\n",
    "    \n",
    "    def fit(self, inputs, labels, epochs):\n",
    "        t_start = time.clock()\n",
    "        print(\"starting fit, time is: \", t_start)\n",
    "        t_last = time.clock()\n",
    "        for i in range(epochs):\n",
    "            print(\"epoch: \", i + 1)\n",
    "            # randomize data\n",
    "            print(np.shape(inputs), np.shape(labels))\n",
    "            state = np.random.get_state()\n",
    "            np.random.shuffle(inputs)\n",
    "            np.random.set_state(state)\n",
    "            np.random.shuffle(labels)\n",
    "            print(np.shape(inputs), np.shape(labels))\n",
    "            self.update_weights(inputs, labels)\n",
    "            t_epoch = time.clock()\n",
    "            print(\"time elapsed is: \", t_epoch - t_last)\n",
    "            \n",
    "    def predict(self, inp, labels):\n",
    "        self.fforw(inp, labels)\n",
    "#         return self.outputs[len(self.nodes) - 2]\n",
    "        return [np.argmax(x) for x in self.outputs[len(self.nodes) - 2]] #output of final layer\n",
    "\n",
    "# General functions\n",
    "\n",
    "def safe_divide(num, denom):\n",
    "    if len(num) != len(denom):\n",
    "        return []\n",
    "    return [num[i] / denom[i] if num[i] > 0.0 and denom[i] > 0.0 else 0.0 for i in range(len(num))]\n",
    "\n",
    "def score(preds, targets, nclasses):\n",
    "    # calculate fscore for each class, then take macro average (unweighted)\n",
    "    if (len(preds) != len(targets)):\n",
    "        return\n",
    "    true_pos = [0.0] * nclasses\n",
    "    pos = [0.0] * nclasses\n",
    "    rel_pos = [0.0] * nclasses\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] == targets[i]:\n",
    "            true_pos[targets[i]] += 1.0\n",
    "        pos[preds[i]] += 1.0\n",
    "        rel_pos[targets[i]] += 1.0\n",
    "    \n",
    "    true_pos = np.array(true_pos)\n",
    "    pos = np.array(pos)\n",
    "    rel_pos = np.array(rel_pos)\n",
    "    print(true_pos, pos, rel_pos)\n",
    "    p = safe_divide(true_pos, pos)\n",
    "    r = safe_divide(true_pos, rel_pos)\n",
    "    print(p, r)\n",
    "    \n",
    "    f_scores = [2.0 * p[i] * r[i] / (p[i] + r[i]) if p[i] + r[i] > 0.0 else 0.0 for i in range(len(p))]\n",
    "    \n",
    "    return np.mean(f_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example of 1 hidden layer, 1 output layer NN. \n",
    "# Input is 2 features, nb of classes is 5 in this case\n",
    "n = Network([3,3,3])\n",
    "\n",
    "# Updates the weights given a mini-batch of 2 examples in this case \n",
    "    #Eg: [1,2] are the features for the first example, [3,4] are the features for the second example\n",
    "    #[0,4] represents the labels of the given examples, where 0 means [1,2] represents class 1, \n",
    "    # and 4 means [3,4] is class 5.\n",
    "# n.update_weights([[1,2], [3,4]], np.array([0,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Example\n",
      "We train the network on the following: \n",
      "data:  [[1, 3, 1], [3, 5, 1], [5, 4, 2], [2, 1, 0], [0, 4, 0], [2, 3, 0], [3, 2, 1], [4, 1, 0]] \n",
      "targets:  [0, 1, 2, 1, 2, 0, 1, 2]\n",
      "starting fit, time is:  173.62673917191555\n",
      "epoch:  1\n",
      "(8, 3) (8,)\n",
      "(8, 3) (8,)\n",
      "time elapsed is:  0.007375399875996891\n",
      "Results:\n",
      "[0. 2. 0.] [0. 6. 0.] [2. 2. 2.]\n",
      "[0.0, 0.3333333333333333, 0.0] [0.0, 1.0, 0.0]\n",
      "tests:  [[1, 0, 1], [2, 3, 1], [3, 1, 0], [4, 1, 0], [5, 3, 0], [0, 1, 2]] predictions:  [1, 1, 1, 1, 1, 1] score:  0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Example\")\n",
    "data = [[1, 3, 1], [3, 5, 1], [5, 4, 2], [2, 1, 0],[0, 4, 0],[2, 3, 0], [3, 2, 1], [4, 1, 0]]\n",
    "targets = [0,1,2,1,2,0,1,2]\n",
    "print(\"We train the network on the following: \")\n",
    "print(\"data: \", data, \"\\ntargets: \", targets)\n",
    "n.fit(data, np.array(targets), 1)\n",
    "print(\"Results:\")\n",
    "test = [[1, 0, 1], [2,3,1], [3,1,0], [4,1,0], [5,3,0], [0,1,2]]\n",
    "test_targ = [2, 2, 0, 1, 0, 1]\n",
    "preds = n.predict(test, np.array(test_targ))\n",
    "print(\"tests: \", test, \"predictions: \", preds, \"score: \", score(preds, test_targ, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.loadtxt(\"../data/train_x.csv\", delimiter=\",\")\n",
    "# y = np.loadtxt(\"../data/train_y.csv\", delimiter=\",\")\n",
    "# x = x.reshape(-1, 64*64)\n",
    "# y = y.reshape(-1, 1)\n",
    "\n",
    "# binary_pics = (x > 250) + 0\n",
    "\n",
    "# NN = Network([4096, 4096, 10])\n",
    "# NN.fit(binary_pics, y, 1)\n",
    "\n",
    "epochs = [1, 5, 25, 125, 625]\n",
    "nhidden = [1, 2, 3]\n",
    "hiddensize = [5, 50, 500, 4096]\n",
    "\n",
    "def tune(epochs, nhidden, hiddensize, train_x, train_y, test_x, test_y):\n",
    "    res = []\n",
    "    for nepochs in epochs:\n",
    "        for layers in nhidden:\n",
    "            for size in hiddensize:\n",
    "                nodes = [4096]\n",
    "                for layer in layers:\n",
    "                    nodes.append(size)\n",
    "                nodes.append(10)\n",
    "                NN = Network(nodes)\n",
    "                NN.fit(train_x, train_y, nepochs)\n",
    "                preds = NN.predict(test_x, test_y)\n",
    "                s = score(preds, test_y, 4096)\n",
    "                res.append((nepochs, layers, size, score))\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
