{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        nodes,\n",
    "        step_size=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        nodes represents the number of nodes per layer. \n",
    "        eg: [2,3,5] is 2 feature input, \n",
    "        3 neurons in the first layer,\n",
    "        5 neurons in the final layer.\n",
    "        5 also represents the number of classes\n",
    "        \n",
    "        nb_layers includes both the output and input layers\n",
    "        outputs is the output matrix of each layer. An output matrix is of shape (p, n),\n",
    "            where p is the number of examples given to the feedforward, and n is the nb of nodes in the layer\n",
    "        derivates is the derivates of each layer. Each row is a different layer\n",
    "        errod_ds is the derivate of the error function\n",
    "        \n",
    "        Default cost function set to cross entropy, therefore, weights[-1] == nb of classes\n",
    "        \"\"\"\n",
    "        self.step_size = step_size\n",
    "        \n",
    "        self.nb_layers = len(nodes)\n",
    "        self.nodes = nodes\n",
    "        self.weights = [ np.ones((n+1, m), dtype=float) for n,m in zip(nodes[:-1], nodes[1:])]\n",
    "        self.outputs = [ 0 for n in nodes[1:]]\n",
    "        self.derivatives = [ 0 for n in nodes[1:]]\n",
    "        self.error_ds = np.zeros(nodes[-1])\n",
    "        self.gradients = [ 0 for n in nodes[1:]]\n",
    "        \n",
    "    def fforw(self, inputs, labels):\n",
    "        shape = np.shape(inputs)\n",
    "        cur_input = np.ones((shape[0], shape[1] + 1))\n",
    "        cur_input[:, :-1] = inputs\n",
    "        for l,w in enumerate(self.weights):\n",
    "            out = self.sigmoid(np.dot(cur_input, w))\n",
    "            self.outputs[l] = out\n",
    "            self.derivatives[l] = self.sigmoid(out, True)\n",
    "            \n",
    "            shape = np.shape(out)\n",
    "            cur_input = np.ones((shape[0], shape[1] + 1))\n",
    "            cur_input[:, :-1] = out\n",
    "        \n",
    "        #Calculating the derivative of the error function for backprop\n",
    "        self.error_ds = self.softmaxLoss(cur_input[:, :-1], labels)\n",
    "        \n",
    "        #appending the input as output[-1] for future use\n",
    "        self.outputs.append(inputs)\n",
    "        \n",
    "        return   \n",
    "\n",
    "    def backprop(self):\n",
    "        \"\"\"\n",
    "        Each gradient is of shape p x m,\n",
    "            where p is the number of examples, m is the number of output nodes from the layer\n",
    "        \"\"\"\n",
    "        self.gradients[-1] = self.derivatives[-1] * self.error_ds\n",
    "        \n",
    "        for i in range(1, len(self.derivatives)):\n",
    "            index = len(self.derivatives) - 1 - i\n",
    "            example_gradients = np.zeros(np.shape(self.derivatives[index]))\n",
    "            for j,example in enumerate(self.derivatives[index]):\n",
    "                example_gradients[j] = np.dot(np.diag(self.derivatives[index][j]), self.weights[index + 1][:-1]).dot(self.gradients[index+1][j])        \n",
    "            self.gradients[index] = example_gradients\n",
    "    \n",
    "    def update_weights(self, inputs, labels):\n",
    "        \"\"\"\n",
    "        inputs is the given input for the network.\n",
    "        Shape of inputs should be (n x m)\n",
    "            Where n is the number of examples,\n",
    "            m is the number of features\n",
    "        labels are the correct labels for each example of shape (n,)\n",
    "        \"\"\"\n",
    "        self.fforw(inputs, labels)\n",
    "        self.backprop()\n",
    "        for i,w in enumerate(self.weights):\n",
    "            shape = np.shape(self.outputs[i-1])\n",
    "            hat_o = np.ones((shape[0], shape[1] + 1))\n",
    "            hat_o [:, :-1] = self.outputs[i-1]\n",
    "            for e,g in enumerate(self.gradients[i]):\n",
    "                single_grad = self.gradients[i][e]\n",
    "                single_grad.shape = (len(single_grad), 1)\n",
    "                single_hat_o = hat_o[e]\n",
    "                single_hat_o.shape = (1, len(single_hat_o))\n",
    "                single_update = -self.step_size*(np.dot(single_grad, single_hat_o)).T\n",
    "                self.weights[i] += single_update\n",
    "    \n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        if derivative:\n",
    "            return x*(1-x)\n",
    "        else:\n",
    "            return 1/(1+np.exp(-x))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def softmaxLoss(self, X, y):\n",
    "        m = y.shape[0]\n",
    "        p = softmax(X)\n",
    "#         log_likelihood = -np.log(p[range(m), y])\n",
    "#         loss = np.sum(log_likelihood) / m\n",
    "\n",
    "        dx = p.copy()\n",
    "        dx[range(m), y] -= 1\n",
    "        dx /= m\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Example of 1 hidden layer, 1 output layer NN. \n",
    "#Input is 2 features, nb of classes is 5 in this case\n",
    "n = Network([2,3,5])\n",
    "\n",
    "#Updates the weights given a mini-batch of 2 examples in this case \n",
    "    #Eg: [1,2] are the features for the first example, [3,4] are the features for the second example\n",
    "    #[0,4] represents the labels of the given examples, where 0 means [1,2] represents class 1, \n",
    "    # and 4 means [3,4] is class 5.\n",
    "n.update_weights([[1,2], [3,4]], np.array([0,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
