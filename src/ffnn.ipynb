{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "stdout = sys.stdout\n",
    "import numpy as np\n",
    "from numpy.random import shuffle\n",
    "sys.stdout = stdout\n",
    "import time\n",
    "print(\"started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        nodes,\n",
    "        step_size=0.01\n",
    "    ):\n",
    "        \"\"\"\n",
    "        nodes represents the number of nodes per layer. \n",
    "        eg: [2,3,5] is 2 feature input, \n",
    "        3 neurons in the first layer,\n",
    "        5 neurons in the final layer.\n",
    "        5 also represents the number of classes\n",
    "        \n",
    "        nb_layers includes both the output and input layers\n",
    "        outputs is the output matrix of each layer. An output matrix is of shape (p, n),\n",
    "            where p is the number of examples given to the feedforward, and n is the nb of nodes in the layer\n",
    "        derivates is the derivates of each layer. Each row is a different layer\n",
    "        errod_ds is the derivate of the error function\n",
    "        \n",
    "        Default cost function set to cross entropy, therefore, weights[-1] == nb of classes\n",
    "        \"\"\"\n",
    "        self.step_size = step_size\n",
    "        \n",
    "        self.nb_layers = len(nodes)\n",
    "        self.nodes = nodes\n",
    "        self.weights = [ np.random.rand(n, m) for n,m in zip(nodes[:-1], nodes[1:])]\n",
    "#         print([np.shape(x) for x in self.weights])\n",
    "        temp = [np.shape(x) for x in self.weights[1:]]\n",
    "        temp.append((self.nodes[-1],1))\n",
    "#         print(temp)\n",
    "        self.biases = [np.random.rand(1,n) for n in self.nodes[1:]]\n",
    "        self.outputs = [ 0 for n in nodes[1:]]\n",
    "        self.derivatives = [ 0 for n in nodes[1:]]\n",
    "        self.error_ds = np.zeros(nodes[-1])\n",
    "        self.gradients = [ 0 for n in nodes[1:]]\n",
    "        self.deltas = [ 0 for n in nodes[1: ]]\n",
    "    \n",
    "    def get_bias_matrix(index, nsamples):\n",
    "        b = self.biases[index]\n",
    "    \n",
    "    def fforw(self, inputs, labels):\n",
    "        curr_input = inputs\n",
    "        for w in range(self.nb_layers-1):\n",
    "            next_inp = curr_input.dot(self.weights[w])\n",
    "            next_inp += self.biases[w]\n",
    "#             print(trans_bias.shape, self.biases[w].shape)\n",
    "#             print(next_inp.shape)\n",
    "            next_act = self.relu(next_inp)\n",
    "            self.outputs[w] = next_act\n",
    "            curr_input = next_act\n",
    "    \n",
    "    def backprop(self, inputs, labels):\n",
    "        # using MSE loss, calculate error\n",
    "        err = np.zeros((len(labels), self.nodes[-1]), dtype=float)\n",
    "        for i,label in enumerate(labels):\n",
    "            expected = int(label[0])\n",
    "            err[i][expected] = label[0]\n",
    "        err = err - self.outputs[-1]\n",
    "#         print(err.shape)\n",
    "        # calculate gradients\n",
    "        for i in range(self.nb_layers-1):\n",
    "            self.gradients[i] = self.relu(self.outputs[i], True)\n",
    "        \n",
    "        # calculate delta\n",
    "        self.deltas[self.nb_layers-2] = err * self.gradients[self.nb_layers-2]\n",
    "        \n",
    "        for i in reversed(range(0, self.nb_layers-2)):\n",
    "#             print(self.deltas[i+1].shape, self.weights[i+1].shape, self.gradients[i].shape)\n",
    "            self.deltas[i] = (self.deltas[i+1].dot(self.weights[i+1].T)) * self.gradients[i]\n",
    "        \n",
    "        # update weights\n",
    "        for i in reversed(range(1, self.nb_layers-1)):\n",
    "            update = (-self.step_size)*self.outputs[i-1].T.dot(self.deltas[i])\n",
    "            self.weights[i] += update\n",
    "        self.weights[0] += (-self.step_size)*inputs.T.dot(self.deltas[0])\n",
    "        \n",
    "        # update biases\n",
    "        for i in range(self.nb_layers-1):\n",
    "            bias_update = np.sum(self.deltas[i], axis=0) * (-self.step_size)\n",
    "#             print(\"bias\",self.biases[i].shape, bias_update.shape)\n",
    "            self.biases[i] = self.biases[i] + bias_update\n",
    "    \n",
    "    def train(self, inputs, labels):\n",
    "        self.fforw(inputs, labels)\n",
    "        self.backprop(inputs, labels)\n",
    "        \n",
    "    def relu(self, x, derivative=False):\n",
    "        if derivative:\n",
    "            x[x<=0.0] = 0.0\n",
    "            x[x>0.0] = 1.0\n",
    "            return x\n",
    "        else:\n",
    "            return x * (x > 0)\n",
    "    \n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        if derivative:\n",
    "            return x*(1.0-x)\n",
    "        else:\n",
    "            return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "    def fit(self, inputs, labels, epochs):\n",
    "        t_start = time.clock()\n",
    "        print(\"starting fit, time is: \", t_start)\n",
    "        t_last = time.clock()\n",
    "        for i in range(epochs):\n",
    "            print(\"epoch: \", i + 1)\n",
    "            # randomize data\n",
    "            print(np.shape(inputs), np.shape(labels))\n",
    "            state = np.random.get_state()\n",
    "            np.random.shuffle(inputs)\n",
    "            np.random.set_state(state)\n",
    "            np.random.shuffle(labels)\n",
    "            print(np.shape(inputs), np.shape(labels))\n",
    "#             self.update_weights(inputs, labels)\n",
    "            self.train(inputs, labels)\n",
    "            t_epoch = time.clock()\n",
    "            print(\"time elapsed is: \", t_epoch - t_last)\n",
    "            \n",
    "    def predict(self, inp, labels):\n",
    "        self.fforw(inp, labels)\n",
    "#         return self.outputs[len(self.nodes) - 2]\n",
    "#         print(\"outputs len: \", len(self.outputs))\n",
    "#         print(\"outputs\", self.outputs[-1])\n",
    "        print(\"outputs\", self.outputs[self.nb_layers-2][0])\n",
    "        return [np.argmax(x) for x in self.outputs[self.nb_layers-2]]\n",
    "        #output of final layer\n",
    "\n",
    "# General functions\n",
    "\n",
    "def safe_divide(num, denom):\n",
    "    if len(num) != len(denom):\n",
    "        return []\n",
    "    return [num[i] / denom[i] if num[i] > 0.0 and denom[i] > 0.0 else 0.0 for i in range(len(num))]\n",
    "\n",
    "def score(preds, targets, nclasses):\n",
    "    # calculate fscore for each class, then take macro average (unweighted)\n",
    "    if (len(preds) != len(targets)):\n",
    "        return -1.0\n",
    "    true_pos = [0.0] * nclasses\n",
    "    pos = [0.0] * nclasses\n",
    "    rel_pos = [0.0] * nclasses\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] == targets[i]:\n",
    "            true_pos[int(targets[i])] += 1.0\n",
    "        pos[int(preds[i])] += 1.0\n",
    "        rel_pos[int(targets[i])] += 1.0\n",
    "    \n",
    "    true_pos = np.array(true_pos)\n",
    "    pos = np.array(pos)\n",
    "    rel_pos = np.array(rel_pos)\n",
    "    print(\"true_pos\", true_pos, \"\\n\", \"pos\", pos, \"\\n\", \"rel_pos\", rel_pos, \"\\n\")\n",
    "    p = safe_divide(true_pos, pos)\n",
    "    r = safe_divide(true_pos, rel_pos)\n",
    "    print(p, r)\n",
    "    \n",
    "    f_scores = [2.0 * p[i] * r[i] / (p[i] + r[i]) if p[i] + r[i] > 0.0 else 0.0 for i in range(len(p))]\n",
    "    \n",
    "    return np.mean(f_scores)\n",
    "\n",
    "def acc(preds, targets):\n",
    "    if (len(preds) != len(targets)):\n",
    "        return -1.0\n",
    "    correct = 0\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] == targets[i]:\n",
    "            correct += 1\n",
    "    return float(correct) / float(len(preds))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [4096, 2048, 10]\n",
    "print([(n, m) for n, m in zip(nodes[:-1], nodes[1:])])\n",
    "# print([ np.random.rand(n, m) for n,m in zip(nodes[:-1], nodes[1:])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.clock()\n",
    "print(\"Loading train_x.npy, elapsed time: \", time.clock() - start_time)\n",
    "# x = np.loadtxt(\"../data/train_x.csv\", delimiter=\",\", dtype=int)\n",
    "x = np.load(\"../data/x_centred_train.npy\")\n",
    "print(\"Loaded train_x.csv. Loading train_y.csv, elapsed time: \", time.clock() - start_time)\n",
    "y = np.loadtxt(\"../data/train_y.csv\", delimiter=\",\", dtype=int)\n",
    "print(y.shape)\n",
    "x = x.reshape(-1, 64*64)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# print(type(y[0][0]), y[0].shape, y.shape)\n",
    "\n",
    "# binary_pics = (x > 250) + 0\n",
    "\n",
    "print(np.shape(x), np.shape(y))\n",
    "print(x[5000][2053], type(x[5000][2053]))\n",
    "\n",
    "#shuffle x and y\n",
    "state = np.random.get_state()\n",
    "np.random.shuffle(x)\n",
    "np.random.set_state(state)\n",
    "np.random.shuffle(y)\n",
    "\n",
    "#make train/valid split\n",
    "train_size = 35000\n",
    "valid_size = 1000\n",
    "end = train_size + valid_size\n",
    "train_x = x[0:train_size]\n",
    "train_y = y[0:train_size]\n",
    "valid_x = x[train_size: end]\n",
    "valid_y = y[train_size: end]\n",
    "\n",
    "print(np.shape(train_x), np.shape(train_y), np.shape(valid_x), np.shape(valid_y))\n",
    "\n",
    "print(x.shape, type(x[500]), type(x[500][50]))\n",
    "\n",
    "for i in range(0, train_size):\n",
    "    for j in range(0, 4096):\n",
    "        if (np.isnan(train_x[i][j])):\n",
    "            print(i, j, \"NaN found\")\n",
    "        if (type(train_x[i][j]) != np.float64):\n",
    "            print(i, j, type(train_x[i][j]), \"non float\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = Network([4096, 2048, 10])\n",
    "NN.train(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Training NN, elapsed time: \", time.clock() - start_time)\n",
    "NN = Network([4096, 2048, 10])\n",
    "NN.fit(train_x, train_y, 3)\n",
    "\n",
    "print(\"NN trained. elapsed time: \", time.clock() -start_time)\n",
    "\n",
    "\n",
    "\n",
    "def tune(maxepochs, nhidden, hiddensize, train_x, train_y, test_x, test_y):\n",
    "    res = []\n",
    "    for layers in nhidden:\n",
    "        for size in hiddensize:\n",
    "            nodes = [4096]\n",
    "            for layer in layers:\n",
    "                nodes.append(size)\n",
    "            nodes.append(10)\n",
    "            NN = Network(nodes)\n",
    "            for i in range(1, maxepochs):\n",
    "                NN.fit(train_x, train_y, 1)\n",
    "                preds = NN.predict(test_x, test_y)\n",
    "                s = score(preds, test_y, 10)\n",
    "                a = acc(preds, test_y)\n",
    "                res.append((i, layers, size, score))\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(NN.weights))\n",
    "def nonzerocount(e):\n",
    "    count = 0\n",
    "    for entry in e:\n",
    "        if entry == 0.0 or entry == 0:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# print([nonzerocount(e) for e in valid_x])\n",
    "print(\"preds:\")\n",
    "preds = NN.predict(valid_x, valid_y)\n",
    "print(\"score:\")\n",
    "print(\"fscore: \", score(preds, valid_y, 10))\n",
    "print(\"acc: \", acc(preds, valid_y))\n",
    "epochs = 8\n",
    "nhidden = [1, 2]\n",
    "hiddensize = [1, 5, 50, 500, 2048, 4096]\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([train_x[982][i] for i in range(0,4096)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
